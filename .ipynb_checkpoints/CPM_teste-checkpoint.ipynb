{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import os\n",
    "import warnings\n",
    "import glob\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_matrices(data_dir, file_suffix=None, zscore=False):\n",
    "    \"\"\"\n",
    "    Reads in a set of individual-subject connectivity matrices stored in data_dir,\n",
    "    \n",
    "    Returns a dataframe that is subjects x edges (by vectorizing the upper triangle of each FC matrix).\n",
    "    \n",
    "    Assumes:\n",
    "    - each matrix is stored in a separate file beginning with the subject ID, and\n",
    "    - matrices are symmetric (squareform); i.e., for a parcellation with 268 nodes, matrices should be 268 x 268\n",
    "    \"\"\"\n",
    "    \n",
    "    all_fc_data = {}\n",
    "    for subj in glob.glob(data_dir + \"*.txt\"):\n",
    "        # try to find this subject's matrix\n",
    "        file = subj\n",
    "#         if file_suffix:\n",
    "#             file = [f for f in os.listdir(data_dir) if subj in f and file_suffix in f]\n",
    "#         else:\n",
    "#             file = [f for f in os.listdir(data_dir) if subj in f]\n",
    "        # make sure there is one and only one file    \n",
    "#         if len(file) ==0:\n",
    "#             raise ValueError(\"No data found for subject {}\".format(subj))\n",
    "#         if len(file) >1:\n",
    "#             raise ValueError(\"More than one matrix found for subject {}! Specify a suffix?\".format(subj))\n",
    "        \n",
    "        # read it in and make sure it's symmetric and has reasonable dimensions\n",
    "        print(file)\n",
    "        tmp = np.loadtxt(file,  delimiter=',')\n",
    "        assert tmp.shape[0]==tmp.shape[1]>1, \"Matrix seems to have incorrect dimensions: {}\".format(tmp.shape)\n",
    "        subject = next((s for s in file.split(\"/\")[-1].split('.') if file_suffix in s), None) # returns 'AMBAC070' por exemplo\n",
    "\n",
    "        # take just the upper triangle and store it in a dictionary\n",
    "        if ~zscore:\n",
    "            all_fc_data[subject] = tmp[np.triu_indices_from(tmp, k=1)]\n",
    "        if zscore:\n",
    "            all_fc_data[subject] = sp.stats.zscore(tmp[np.triu_indices_from(tmp, k=1)])\n",
    "    \n",
    "    # Convert dictionary into dataframe\n",
    "    all_fc_data = pd.DataFrame.from_dict(all_fc_data, orient='index')\n",
    "    \n",
    "    return all_fc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-efb28e757442>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"correlation_matrix/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mall_fc_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_in_matrices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile_suffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'AMBAC'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mall_fc_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_fc_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'last'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'index'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-1b6e832c1ed0>\u001b[0m in \u001b[0;36mread_in_matrices\u001b[0;34m(data_dir, file_suffix, zscore)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# read it in and make sure it's symmetric and has reasonable dimensions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Matrix seems to have incorrect dimensions: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0msubject\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfile_suffix\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# returns 'AMBAC070' por exemplo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "path = \"correlation_matrix/\"\n",
    "all_fc_data = read_in_matrices(path, file_suffix='AMBAC')\n",
    "all_fc_data = all_fc_data.reset_index().drop_duplicates(subset='index', keep='last').set_index('index').sort_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fc_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 0\n",
    "\n",
    "sns.heatmap(sp.spatial.distance.squareform(all_fc_data.iloc[s,:]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_kfold_indices(subj_list, k = 10):\n",
    "    \"\"\"\n",
    "    Splits list of subjects into k folds for cross-validation.\n",
    "    \"\"\"\n",
    "    \n",
    "    n_subs = len(subj_list)\n",
    "    n_subs_per_fold = n_subs//k # floor integer for n_subs_per_fold\n",
    "\n",
    "    indices = [[fold_no]*n_subs_per_fold for fold_no in range(k)] # generate repmat list of indices\n",
    "    remainder = n_subs % k # figure out how many subs are left over\n",
    "    remainder_inds = list(range(remainder))\n",
    "    indices = [item for sublist in indices for item in sublist]    \n",
    "    [indices.append(ind) for ind in remainder_inds] # add indices for remainder subs\n",
    "\n",
    "    assert len(indices)==n_subs, \"Length of indices list does not equal number of subjects, something went wrong\"\n",
    "\n",
    "    np.random.shuffle(indices) # shuffles in place\n",
    "\n",
    "    return np.array(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(subj_list, indices, test_fold):\n",
    "    \"\"\"\n",
    "    For a subj list, k-fold indices, and given fold, returns lists of train_subs and test_subs\n",
    "    \"\"\"\n",
    "\n",
    "    train_inds = np.where(indices!=test_fold)\n",
    "    test_inds = np.where(indices==test_fold)\n",
    "\n",
    "    train_subs = []\n",
    "    for sub in subj_list[train_inds]:\n",
    "        train_subs.append(sub)\n",
    "\n",
    "    test_subs = []\n",
    "    for sub in subj_list[test_inds]:\n",
    "        test_subs.append(sub)\n",
    "\n",
    "    return (train_subs, test_subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test_data(all_fc_data, train_subs, test_subs, behav_data, behav):\n",
    "\n",
    "    \"\"\"\n",
    "    Extracts requested FC and behavioral data for a list of train_subs and test_subs\n",
    "    \"\"\"\n",
    "\n",
    "    train_vcts = all_fc_data.loc[train_subs, :]\n",
    "    test_vcts = all_fc_data.loc[test_subs, :]\n",
    "\n",
    "    train_behav = behav_data.loc[train_subs, behav]\n",
    "\n",
    "    return (train_vcts, train_behav, test_vcts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features(train_vcts, train_behav, r_thresh=0.2, corr_type='pearson', verbose=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Runs the CPM feature selection step: \n",
    "    - correlates each edge with behavior, and returns a mask of edges that are correlated above some threshold, one for each tail (positive and negative)\n",
    "    \"\"\"\n",
    "\n",
    "    assert train_vcts.index.equals(train_behav.index), \"Row indices of FC vcts and behavior don't match!\"\n",
    "\n",
    "    # Correlate all edges with behav vector\n",
    "    if corr_type =='pearson':\n",
    "        cov = np.dot(train_behav.T - train_behav.mean(), train_vcts - train_vcts.mean(axis=0)) / (train_behav.shape[0]-1)\n",
    "        corr = cov / np.sqrt(np.var(train_behav, ddof=1) * np.var(train_vcts, axis=0, ddof=1))\n",
    "    elif corr_type =='spearman':\n",
    "        corr = []\n",
    "        for edge in train_vcts.columns:\n",
    "            r_val = sp.stats.spearmanr(train_vcts.loc[:,edge], train_behav)[0]\n",
    "            corr.append(r_val)\n",
    "\n",
    "    # Define positive and negative masks\n",
    "    mask_dict = {}\n",
    "    mask_dict[\"pos\"] = corr > r_thresh\n",
    "    mask_dict[\"neg\"] = corr < -r_thresh\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"Found ({}/{}) edges positively/negatively correlated with behavior in the training set\".format(mask_dict[\"pos\"].sum(), mask_dict[\"neg\"].sum())) # for debugging\n",
    "\n",
    "    return mask_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(train_vcts, mask_dict, train_behav):\n",
    "    \"\"\"\n",
    "    Builds a CPM model:\n",
    "    - takes a feature mask, sums all edges in the mask for each subject, and uses simple linear regression to relate summed network strength to behavior\n",
    "    \"\"\"\n",
    "\n",
    "    assert train_vcts.index.equals(train_behav.index), \"Row indices of FC vcts and behavior don't match!\"\n",
    "\n",
    "    model_dict = {}\n",
    "\n",
    "    # Loop through pos and neg tails\n",
    "    X_glm = np.zeros((train_vcts.shape[0], len(mask_dict.items())))\n",
    "\n",
    "    t = 0\n",
    "    for tail, mask in mask_dict.items():\n",
    "        X = train_vcts.values[:, mask].sum(axis=1)\n",
    "        X_glm[:, t] = X\n",
    "        y = train_behav\n",
    "        (slope, intercept) = np.polyfit(X, y, 1)\n",
    "        model_dict[tail] = (slope, intercept)\n",
    "        t+=1\n",
    "\n",
    "    X_glm = np.c_[X_glm, np.ones(X_glm.shape[0])]\n",
    "    model_dict[\"glm\"] = tuple(np.linalg.lstsq(X_glm, y, rcond=None)[0])\n",
    "\n",
    "    return model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_model(test_vcts, mask_dict, model_dict):\n",
    "    \"\"\"\n",
    "    Applies a previously trained linear regression model to a test set to generate predictions of behavior.\n",
    "    \"\"\"\n",
    "\n",
    "    behav_pred = {}\n",
    "\n",
    "    X_glm = np.zeros((test_vcts.shape[0], len(mask_dict.items())))\n",
    "\n",
    "    # Loop through pos and neg tails\n",
    "    t = 0\n",
    "    for tail, mask in mask_dict.items():\n",
    "        X = test_vcts.loc[:, mask].sum(axis=1)\n",
    "        X_glm[:, t] = X\n",
    "\n",
    "        slope, intercept = model_dict[tail]\n",
    "        behav_pred[tail] = slope*X + intercept\n",
    "        t+=1\n",
    "\n",
    "    X_glm = np.c_[X_glm, np.ones(X_glm.shape[0])]\n",
    "    behav_pred[\"glm\"] = np.dot(X_glm, model_dict[\"glm\"])\n",
    "\n",
    "    return behav_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpm_wrapper(all_fc_data, all_behav_data, behav, k=10, **cpm_kwargs):\n",
    "\n",
    "    assert all_fc_data.index.equals(all_behav_data.index), \"Row (subject) indices of FC vcts and behavior don't match!\"\n",
    "\n",
    "    subj_list = all_fc_data.index # get subj_list from df index\n",
    "    \n",
    "    indices = mk_kfold_indices(subj_list, k=k)\n",
    "    \n",
    "    # Initialize df for storing observed and predicted behavior\n",
    "    col_list = []\n",
    "    for tail in [\"pos\", \"neg\", \"glm\"]:\n",
    "        col_list.append(behav + \" predicted (\" + tail + \")\")\n",
    "    col_list.append(behav + \" observed\")\n",
    "    behav_obs_pred = pd.DataFrame(index=subj_list, columns = col_list)\n",
    "    \n",
    "    # Initialize array for storing feature masks\n",
    "    n_edges = all_fc_data.shape[1]\n",
    "    all_masks = {}\n",
    "    all_masks[\"pos\"] = np.zeros((k, n_edges))\n",
    "    all_masks[\"neg\"] = np.zeros((k, n_edges))\n",
    "    \n",
    "    for fold in range(k):\n",
    "        print(\"doing fold {}\".format(fold))\n",
    "        train_subs, test_subs = split_train_test(subj_list, indices, test_fold=fold)\n",
    "        train_vcts, train_behav, test_vcts = get_train_test_data(all_fc_data, train_subs, test_subs, all_behav_data, behav=behav)\n",
    "        mask_dict = select_features(train_vcts, train_behav, **cpm_kwargs)\n",
    "        all_masks[\"pos\"][fold,:] = mask_dict[\"pos\"]\n",
    "        all_masks[\"neg\"][fold,:] = mask_dict[\"neg\"]\n",
    "        model_dict = build_model(train_vcts, mask_dict, train_behav)\n",
    "        behav_pred = apply_model(test_vcts, mask_dict, model_dict)\n",
    "        for tail, predictions in behav_pred.items():\n",
    "            behav_obs_pred.loc[test_subs, behav + \" predicted (\" + tail + \")\"] = predictions\n",
    "            \n",
    "    behav_obs_pred.loc[subj_list, behav + \" observed\"] = all_behav_data[behav]\n",
    "    \n",
    "    return behav_obs_pred, all_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_behav_data = pd.read_csv('adaptado_manualmente_behav_data.csv', dtype={'Subject': str})\n",
    "all_behav_data.set_index('Subject', inplace=True)\n",
    "all_behav_data = all_behav_data.reset_index().drop_duplicates(subset='Subject', keep='last').set_index('Subject').sort_index()\n",
    "\n",
    "print(all_behav_data.shape)\n",
    "all_behav_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "behav = 'PMAT24_A_CR'\n",
    "\n",
    "sns.distplot(all_behav_data[behav])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpm_kwargs = {'r_thresh': 0.2, 'corr_type': 'pearson'} # these are the defaults, but it's still good to be explicit\n",
    "\n",
    "behav_obs_pred, all_masks = cpm_wrapper(all_fc_data, all_behav_data, behav=behav, **cpm_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_masks['pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "numpy.savetxt('positive_matrix.txt', all_masks['pos'], delimiter = ',')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_masks['pos'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_fc_data.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chord \n",
    "from nilearn import datasets\n",
    "import pandas as pd\n",
    "\n",
    "atlas = datasets.fetch_atlas_msdl()\n",
    "# Loading atlas image stored in 'maps'\n",
    "atlas_filename = atlas['maps']\n",
    "# # Loading atlas data stored in 'labels'\n",
    "networks = atlas['networks']\n",
    "labels = atlas['labels']\n",
    "\n",
    "json = pd.Series(labels).to_json(orient='values')\n",
    "\n",
    "# chord.Chord(all_masks['pos'], json).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(all_masks['pos'][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
